{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Lq3pcNrX11GI",
        "laKneGxkxFIy",
        "dcZF0nMAxqyw",
        "gwqWm_fCxw8o",
        "XdNgRly5zzSV",
        "XIr0lkPbz-3f",
        "V4UTZ5Qj0H6V",
        "jF-kqpmu0PWH",
        "0tTLqUH60T85",
        "qMhyLnKN0bRc",
        "CCBQK13X0g8Z",
        "EhM3A6aJ0nkI",
        "AxxMQ7BI0wxy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nephelim74/BeginProg/blob/main/%D0%A1%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80_1_%D0%9A%D0%BE%D0%BC%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%80%D0%B8%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "#!pip install pyspark >> None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWTu0powcs0",
        "outputId": "da34eb4b-2b6a-401c-cbc6-ff67d8b8542c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G23g-BowYAC"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1: Найти среднее значение элементов в RDD"
      ],
      "metadata": {
        "id": "Lq3pcNrX11GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Создание SparkContext**: Сначала создается объект `SparkContext` с параметрами `\"local\"` и `\"Average RDD\"`. `\"local\"` указывает, что Spark будет работать в локальном режиме, а `\"Average RDD\"` - это имя приложения. `SparkContext` является основным входным точкой для любого Spark-приложения и используется для создания RDD и выполнения операций на кластере.\n",
        "\n",
        "2. **Создание RDD**: Затем используется метод `parallelize` объекта `SparkContext` для создания RDD (Resilient Distributed Dataset) из списка `[1, 2, 3, 4, 5]`. RDD - это основная абстракция в Spark, представляющая собой распределенную коллекцию элементов, которые могут быть обработаны параллельно. Метод `parallelize` копирует элементы коллекции для формирования распределенного набора данных, который может быть обработан параллельно.\n",
        "\n",
        "3. **Вычисление среднего значения**: После создания RDD вызывается метод `mean()` для вычисления среднего значения элементов в RDD.\n",
        "\n",
        "Важно отметить, что количество разделов (partitions) для RDD, созданного с помощью `parallelize`, по умолчанию определяется Spark на основе количества ядер в кластере. Однако, можно указать конкретное количество разделов вторым аргументом метода `parallelize`, если это необходимо для оптимизации производительности."
      ],
      "metadata": {
        "id": "H0klN5fj4PtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"Average RDD\")\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "mean_value = rdd.mean()\n",
        "print(\"Среднее значение элементов в RDD:\", mean_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DM-YBmkw5uI",
        "outputId": "6c1e4f9e-ee0f-42bf-fb45-27abca84a2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Среднее значение элементов в RDD: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "active_session = SparkSession.active\n",
        "print(active_session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFVCxmQzzC0-",
        "outputId": "8c0c3086-7482-4f46-fe3f-be4f2e79d3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method SparkSession.active of <class 'pyspark.sql.session.SparkSession'>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "h946SGKVyRkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2: найти наибольший элемент в RDD"
      ],
      "metadata": {
        "id": "laKneGxkxFIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создает экземпляр `SparkContext` с именем приложения \"Max RDD\" и запускает его на локальном режиме. `SparkContext` является основным объектом в Spark, который позволяет создавать RDD (Resilient Distributed Datasets) и выполнять операции над ними.\n",
        "\n",
        "2. Использует метод `parallelize` для создания RDD из списка чисел `[100, 25, 30, 40, 55, 70]`. Метод `parallelize` позволяет распределить локальную коллекцию данных по рабочим узлам в кластере, преобразуя её в распределённый набор данных (RDD), который может быть обработан параллельно.\n",
        "\n",
        "3. Вызывает метод `max()` на созданном RDD, который возвращает максимальное значение из всех элементов RDD."
      ],
      "metadata": {
        "id": "wU2CqWoZ4ZL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"Max RDD\")\n",
        "rdd = sc.parallelize([100, 25, 30, 40, 55, 70])\n",
        "max_value = rdd.max()\n",
        "print(\"Наибольший элемент в RDD:\", max_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8xkx3v6zddv",
        "outputId": "d000670c-6d47-407f-f5de-a34379dc7cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наибольший элемент в RDD: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "OvXXvHbzzjlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3: подсчитать количество элементов, удовлетворяющих определенному условию"
      ],
      "metadata": {
        "id": "dcZF0nMAxqyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создание экземпляра `SparkContext` с именем \"Filter RDD\" и запуском в локальном режиме. `SparkContext` является основным входным точкой для любого приложения Spark и обеспечивает доступ к функциональности Spark, таким как создание RDD (Resilient Distributed Datasets) и выполнение операций на данных.\n",
        "\n",
        "2. Создание RDD из списка чисел от 1 до 10 с помощью метода `parallelize`. RDD представляет собой распределенную коллекцию элементов, которые могут быть обработаны параллельно. В данном случае, список `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` распределяется по кластеру Spark для параллельной обработки.\n",
        "\n",
        "3. Применение фильтрации к RDD с помощью метода `filter`, который принимает функцию-предикат (в данном случае лямбда-функцию) и возвращает новый RDD, содержащий только те элементы, для которых предикат возвращает `True`. В данном случае, фильтрация выполняется для выбора элементов больше 5.\n",
        "\n",
        "4. Подсчет количества элементов в отфильтрованном RDD с помощью метода `count`."
      ],
      "metadata": {
        "id": "1Pc66JPw4if5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"Filter RDD\")\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "filtered_rdd = rdd.filter(lambda x: x > 5)\n",
        "count = filtered_rdd.count()\n",
        "print(\"Количество элементов, больших 5:\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpvC5v0iylts",
        "outputId": "fd365ba3-ee1d-42a4-a751-a07338899c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество элементов, больших 5: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "rlLo54ju1kMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Задание на группировку по ключу:\n",
        "   Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (товар, магазин, количество). Необходимо сгруппировать данные по товару и найти суммарное количество проданных товаров по каждому товару."
      ],
      "metadata": {
        "id": "gwqWm_fCxw8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"advanced\")` создает экземпляр SparkContext, который является точкой входа для любого приложения Spark. В данном случае, SparkContext инициализируется для работы в локальном режиме (`\"local\"`) с именем приложения `\"advanced\"`.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15), ...]` определяет список кортежей, где каждый кортеж содержит информацию о фрукте, магазине и количестве.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в распределенный набор данных (RDD), который может быть обработан параллельно.\n",
        "\n",
        "4. **Группировка и агрегация данных**:\n",
        "   - `grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)` выполняет две операции:\n",
        "     - `map(lambda x: (x[0], x[2]))` преобразует каждый кортеж в RDD, оставляя только первый элемент (название фрукта) и третий элемент (количество) в каждом кортеже.\n",
        "     - `reduceByKey(lambda a, b: a + b)` агрегирует значения по ключу (в данном случае, названию фрукта), суммируя количество для каждого фрукта. Это достигается путем применения функции `lambda a, b: a + b` к значениям, ассоциированным с каждым ключом. Функция `reduceByKey` автоматически выполняет локальную агрегацию на каждом узле перед вычислением глобальных итогов для каждого ключа, что уменьшает объем сетевого трафика.\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `grouped_rdd.collect()` собирает результаты обработки RDD в драйвер программы. В данном случае, это будет список кортежей, где каждый кортеж содержит название фрукта и суммарное количество этого фрукта по всем магазинам.\n",
        "\n",
        "В итоге, код группирует данные по названию фрукта и суммирует количество каждого фрукта по всем магазинам, используя возможности распределенной обработки данных Apache Spark."
      ],
      "metadata": {
        "id": "0TC9sJF74woA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n",
        "        (\"banana\", \"store1\", 20), (\"banana\", \"store2\", 25),\n",
        "        (\"peach\", \"store1\", 5), (\"peach\", \"store2\", 10),\n",
        "        (\"peach\", \"store3\", 25),]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)\n",
        "grouped_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "HVqGm0U1ztEB",
        "outputId": "a9bbe480-23e6-4b54-a947-321d6e571ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Max RDD, master=local) created by __init__ at <ipython-input-65-bd938542dd78>:1 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-63dd3f1f086a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"advanced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"peach\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"peach\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         (\"peach\", \"store3\", 25),]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Max RDD, master=local) created by __init__ at <ipython-input-65-bd938542dd78>:1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "sNRkRCV-1LOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Задание на агрегацию по ключу:\n",
        "\n",
        "   Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (магазин, товар, количество, цена). Необходимо найти общую выручку от продаж каждого товара в каждом магазине."
      ],
      "metadata": {
        "id": "XdNgRly5zzSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"advanced\")` создает экземпляр SparkContext, который является точкой входа для любого приложения Spark. В данном случае, SparkContext настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"advanced\"`, является именем приложения.\n",
        "\n",
        "2. **Создание данных**:\n",
        "   - `data` - это список кортежей, где каждый кортеж содержит информацию о продажах товаров в разных магазинах. Каждый кортеж содержит название магазина, название товара, количество проданных единиц товара и цену за единицу товара.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в распределенный набор данных (RDD), который может быть обработан параллельно. Это достигается путем копирования элементов списка в распределенный набор данных, который затем может быть обработан параллельно.\n",
        "\n",
        "4. **Преобразование и агрегация данных**:\n",
        "   - `revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)` выполняет две операции:\n",
        "     - `map` преобразует каждый элемент RDD, применяя к нему функцию, которая возвращает кортеж, состоящий из пары (магазин, товар) и произведения количества и цены за единицу товара. Это позволяет вычислить общую прибыль от продажи каждого товара в каждом магазине.\n",
        "     - `reduceByKey` агрегирует значения по ключам (в данном случае, по паре (магазин, товар)), суммируя прибыль от продажи каждого товара в каждом магазине. Это делается путем применения функции, которая складывает значения для каждого ключа.\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `revenue_rdd.collect()` собирает результаты обработки данных в машину, на которой запущено приложение. В результате выполнения этого кода будет получен список кортежей, где каждый кортеж содержит пару (магазин, товар) и общую прибыль от продажи этого товара в этом магазине.\n",
        "\n",
        "Важно отметить, что `reduceByKey` автоматически выполняет локальную агрегацию данных на каждом узле перед вычислением глобальных итогов для каждого ключа, что улучшает производительность и снижает объем сетевого трафика."
      ],
      "metadata": {
        "id": "6UCTUSz2445v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "data = [(\"store1\", \"apple\", 10, 2), (\"store2\", \"apple\", 15, 2.5),\n",
        "        (\"store1\", \"banana\", 20, 1.5), (\"store2\", \"banana\", 25, 1.8),\n",
        "        (\"store1\", \"watermelon\", 3, 5), (\"store2\", \"watermelon\", 2, 4)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)\n",
        "revenue_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZHpDf07z2l5",
        "outputId": "c98bedfa-2a4c-4f02-ab7c-a660eec78e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('store1', 'apple'), 20),\n",
              " (('store2', 'apple'), 37.5),\n",
              " (('store1', 'banana'), 30.0),\n",
              " (('store2', 'banana'), 45.0),\n",
              " (('store1', 'watermelon'), 15),\n",
              " (('store2', 'watermelon'), 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "I7J2ptTb1W7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Задание на джоин по ключу:\n",
        "\n",
        "   Даны два набора данных: первый с информацией о продажах (товар, количество) и второй с информацией о цене товаров (товар, цена). Необходимо объединить данные и найти общую выручку от продаж каждого товара."
      ],
      "metadata": {
        "id": "XIr0lkPbz-3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - Создается экземпляр `SparkContext` с именем \"advanced\", который будет использоваться для выполнения операций в Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, используя все доступные ядра на компьютере.\n",
        "\n",
        "2. **Создание RDD (Resilient Distributed Datasets)**:\n",
        "   - Создаются два списка кортежей: `sales_data` и `price_data`, которые содержат информацию о продажах и ценах на фрукты соответственно.\n",
        "   - Используя метод `parallelize` из `SparkContext`, эти списки преобразуются в RDD. RDD - это распределенный набор данных, который может быть обработан параллельно. Это позволяет Spark эффективно распределять данные по кластеру для параллельной обработки.\n",
        "\n",
        "3. **Объединение RDD**:\n",
        "   - Выполняется операция `join` между `sales_rdd` и `price_rdd`. Этот метод объединяет два RDD по ключам, в данном случае по названиям фруктов. Результатом будет новый RDD, где каждый элемент содержит пару (ключ, (значение из `sales_rdd`, значение из `price_rdd`)).\n",
        "\n",
        "4. **Преобразование данных**:\n",
        "   - Применяется функция `map` к объединенному RDD для вычисления дохода от продаж каждого фрукта. Функция принимает кортеж, где первый элемент - это ключ (название фрукта), а второй элемент - кортеж из двух значений: количество проданных фруктов и цена за единицу. Результатом будет новый RDD, где каждый элемент содержит пару (ключ, доход от продаж).\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - Вызывается метод `collect` для получения результатов из RDD в виде списка. Этот метод собирает все элементы RDD в драйвер-программу, что позволяет просмотреть результаты вычислений.\n",
        "\n",
        "В итоге, код вычисляет доходы от продаж для каждого фрукта, используя данные о продажах и ценах, и выводит результаты в виде списка кортежей, где каждый кортеж содержит название фрукта и соответствующий доход."
      ],
      "metadata": {
        "id": "NiKP6PJS5B1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "sales_data = [(\"apple\", 10), (\"banana\", 20), (\"apple\", 15), (\"banana\", 25), (\"peach\", 15), (\"peach\", 25), (\"watermelon\", 5), (\"watermelon\", 10)]\n",
        "price_data = [(\"apple\", 2), (\"banana\", 1.5), (\"peach\", 2.5), (\"watermelon\", 3.5)]\n",
        "sales_rdd = sc.parallelize(sales_data)\n",
        "price_rdd = sc.parallelize(price_data)\n",
        "joined_rdd = sales_rdd.join(price_rdd)\n",
        "revenue_rdd = joined_rdd.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
        "revenue_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8Z3ExW0CJc",
        "outputId": "0e25a345-238f-4f57-bc6a-79da0b5e0181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('banana', 30.0),\n",
              " ('banana', 37.5),\n",
              " ('peach', 37.5),\n",
              " ('peach', 62.5),\n",
              " ('watermelon', 17.5),\n",
              " ('watermelon', 35.0),\n",
              " ('apple', 20),\n",
              " ('apple', 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "JkXOb2Sl1Xg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Поиск самого длинного слова в RDD"
      ],
      "metadata": {
        "id": "V4UTZ5Qj0H6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Создает экземпляр `SparkContext` с именем \"RDD tasks\", который используется для выполнения операций с RDD (Resilient Distributed Datasets) в локальном режиме. Этот контекст необходим для работы с Spark и создания RDD из коллекций данных.\n",
        "\n",
        "2. Определяет список строк `data`, содержащий слова на русском языке.\n",
        "\n",
        "3. Использует метод `parallelize` из `SparkContext` для преобразования локальной коллекции `data` в RDD. Это позволяет распределить данные по кластеру для параллельной обработки. В данном случае, поскольку используется локальный режим, данные будут распределены только внутри одного узла.\n",
        "\n",
        "4. Выполняет операцию `max` на RDD, чтобы найти самое длинное слово из списка. Для этого используется лямбда-функция `lambda x: len(x)`, которая возвращает длину каждого слова. Операция `max` сравнивает длины слов и возвращает самое длинное.\n",
        "\n",
        "В итоге, код позволяет определить самое длинное слово из списка, используя возможности распределенной обработки данных, предоставляемые Spark."
      ],
      "metadata": {
        "id": "iJ6Dscr95J2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "longest_word = rdd.max(key=lambda x: len(x))\n",
        "print(\"Самое длинное слово: \", longest_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWl4FI2U0KY9",
        "outputId": "77179c51-25c2-4c87-d0bd-7e1fc6c24cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Самое длинное слово:  Путеводитель\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "uQtZzBhr1YNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Фильтрация слов по длине в RDD"
      ],
      "metadata": {
        "id": "jF-kqpmu0PWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием Python API (PySpark):\n",
        "\n",
        "1. **Создание SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")`\n",
        "   - Создает экземпляр `SparkContext`, который является основным точкой входа для работы с Spark. В данном случае, контекст настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]`\n",
        "   - Создает список строк, который будет использоваться для создания RDD (Resilient Distributed Datasets).\n",
        "\n",
        "3. **Преобразование списка в RDD**:\n",
        "   - `rdd = sc.parallelize(data)`\n",
        "   - Использует метод `parallelize` из `SparkContext` для преобразования локального списка `data` в RDD. RDD - это распределенная коллекция объектов, которая может быть разделена на несколько партиций для параллельной обработки. В данном случае, `parallelize` автоматически разделяет данные на партиции, распределяя их по узлам в кластере Spark.\n",
        "\n",
        "4. **Фильтрация RDD**:\n",
        "   - `filtered_rdd = rdd.filter(lambda x: len(x) > 6)`\n",
        "   - Применяет функцию фильтрации к RDD, используя лямбда-функцию, которая возвращает `True` для строк длиной более 6 символов. Это приводит к созданию нового RDD, содержащего только те элементы исходного RDD, которые удовлетворяют условию фильтрации.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   - `print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())`\n",
        "   - Выводит на экран список слов, длина которых превышает 6 символов. Метод `collect` собирает все элементы RDD обратно в драйвер-программу, что позволяет их вывести на экран.\n",
        "\n",
        "В итоге, код создает RDD из списка слов, фильтрует его, оставляя только слова длиной более 6 символов, и выводит результат на экран."
      ],
      "metadata": {
        "id": "SEwyAhaF5Set"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "filtered_rdd = rdd.filter(lambda x: len(x) > 6)\n",
        "print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAc8_ldv0QUz",
        "outputId": "68009c6c-4a97-41f8-a5c3-340e0a0ec008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слова длиной более 6 символов:  ['Приложение', 'Путеводитель', 'Метрополитен']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "krNvave71YvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Подсчет количества уникальных слов в RDD"
      ],
      "metadata": {
        "id": "0tTLqUH60T85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием Python API (PySpark):\n",
        "\n",
        "1. **Создание SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")`\n",
        "   - Создает экземпляр `SparkContext`, который является основным точкой входа для работы с Spark. В данном случае, контекст настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]`\n",
        "   - Создает список строк, который будет использоваться для создания RDD (Resilient Distributed Datasets).\n",
        "\n",
        "3. **Преобразование списка в RDD**:\n",
        "   - `rdd = sc.parallelize(data)`\n",
        "   - Использует метод `parallelize` из `SparkContext` для преобразования локального списка `data` в RDD. RDD - это распределенная коллекция объектов, которая может быть разделена на несколько партиций для параллельной обработки. В данном случае, `parallelize` автоматически разделяет данные на партиции, распределяя их по узлам в кластере Spark [0][3].\n",
        "\n",
        "4. **Фильтрация RDD**:\n",
        "   - `filtered_rdd = rdd.filter(lambda x: len(x) > 6)`\n",
        "   - Применяет функцию фильтрации к RDD, используя лямбда-функцию, которая возвращает `True` для строк длиной более 6 символов. Это приводит к созданию нового RDD, содержащего только те элементы исходного RDD, которые удовлетворяют условию фильтрации.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   - `print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())`\n",
        "   - Выводит на экран список слов, длина которых превышает 6 символов. Метод `collect` собирает все элементы RDD обратно в драйвер-программу, что позволяет их вывести на экран.\n",
        "\n",
        "В итоге, код создает RDD из списка слов, фильтрует его, оставляя только слова длиной более 6 символов, и выводит результат на экран."
      ],
      "metadata": {
        "id": "Zlb6yBdJ5bUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\",\n",
        "        \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\", \"Яблоко\", \"Путеводитель\", \"Анализ\"]\n",
        "rdd = sc.parallelize(data)\n",
        "unique_words_count = rdd.distinct().count()\n",
        "print(\"Количество уникальных слов \", unique_words_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCHo2mZa0VJw",
        "outputId": "45b906fd-bc17-4aa7-b444-2b9d65d38c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество уникальных слов  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "Nlog_6RW1ZNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Преобразование всех слов в RDD в верхний регистр"
      ],
      "metadata": {
        "id": "qMhyLnKN0bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием PySpark:\n",
        "\n",
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")` создает экземпляр SparkContext, который является основным входным точкой для любого приложения Spark. В данном случае, SparkContext инициализируется для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]` определяет список строк, который будет использоваться в качестве исходных данных.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в RDD (Resilient Distributed Dataset), что позволяет Spark распределить данные по нескольким узлам для параллельной обработки. RDD представляет собой неизменяемую, распределенную коллекцию элементов, которые могут быть обработаны параллельно [2].\n",
        "\n",
        "4. **Преобразование данных**:\n",
        "   - `upper_rdd = rdd.map(lambda x: x.upper())` применяет функцию `map` к RDD, которая преобразует каждый элемент списка в верхний регистр. Функция `map` применяется к каждому элементу RDD и возвращает новый RDD, содержащий результаты преобразования. В данном случае, используется лямбда-функция, которая принимает один аргумент `x` и возвращает `x.upper()`, преобразуя каждую строку в верхний регистр [4].\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `print(\"Слова в верхнем регистре \", upper_rdd.collect())` собирает результаты обработки из RDD `upper_rdd` и выводит их. Метод `collect` собирает все элементы RDD обратно на драйвер-узел в виде списка. Это полезно для отладки или когда необходимо получить результаты обработки на драйвер-узеле [0][2].\n",
        "\n",
        "В целом, код демонстрирует базовые операции с RDD в Spark: инициализацию SparkContext, создание RDD из локальной коллекции, применение трансформации к RDD и сбор результатов обработки."
      ],
      "metadata": {
        "id": "DwV_-H_f5lQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "upper_rdd = rdd.map(lambda x: x.upper())\n",
        "print(\"Слова в верхнем регистре \", upper_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpjSNgAF0cF0",
        "outputId": "06345723-4925-4a33-ad0f-61ba401ba174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слова в верхнем регистре  ['ПРИЛОЖЕНИЕ', 'ЯБЛОКО', 'СПАРК', 'ПУТЕВОДИТЕЛЬ', 'МЕТРОПОЛИТЕН', 'АНАЛИЗ', 'СОЛНЦЕ', 'ПИТОН', 'СНЕГ', 'РЫНОК']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "zlPSZbuN1ZyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Найти средний возраст пользователей по их покупкам и вывести топ-5 самых молодых пользователей."
      ],
      "metadata": {
        "id": "CCBQK13X0g8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark для обработки данных о возрасте пользователей и их покупках:\n",
        "\n",
        "1. **Инициализация SparkContext**:\n",
        "   ```python\n",
        "   sc = SparkContext(\"local\", \"UserAge\")\n",
        "   ```\n",
        "   Создается экземпляр `SparkContext`, который является точкой входа для любого приложения Spark. В данном случае, приложение запускается локально, и ему присваивается имя \"UserAge\".\n",
        "\n",
        "2. **Создание RDD**:\n",
        "   ```python\n",
        "   user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
        "   ```\n",
        "   Создается RDD (Resilient Distributed Dataset) с именем `user_purchase_rdd`, содержащий пары (user_id, age), где `user_id` - идентификатор пользователя, а `age` - его возраст. Метод `parallelize` используется для создания RDD из списка пар.\n",
        "\n",
        "3. **Преобразование и агрегация данных**:\n",
        "   ```python\n",
        "   user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "   ```\n",
        "   - `mapValues(lambda age: (age, 1))` преобразует каждую пару (user_id, age) в пару (user_id, (age, 1)), где второй элемент является кортежем, содержащим возраст и счетчик единиц.\n",
        "   - `reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))` агрегирует значения по ключам (user_id), суммируя возраст и счетчик единиц для каждого пользователя.\n",
        "   - `mapValues(lambda v: v[0] / v[1])` вычисляет средний возраст для каждого пользователя, разделив сумму возрастов на количество записей.\n",
        "\n",
        "4. **Сортировка и выборка**:\n",
        "   ```python\n",
        "   youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
        "   ```\n",
        "   - `sortBy(lambda x: x[1])` сортирует пары (user_id, avg_age) по возрастанию среднего возраста.\n",
        "   - `take(5)` выбирает первые 5 пользователей по возрастанию среднего возраста.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   ```python\n",
        "   for user_id, avg_age in youngest_users:\n",
        "       print(user_id, avg_age)\n",
        "   ```\n",
        "   Выводит идентификаторы и средние возрасты пяти самых молодых пользователей.\n",
        "\n",
        "В целом, код выполняет агрегацию данных о возрасте пользователей, вычисляет средний возраст для каждого пользователя и выводит идентификаторы и средние возрасты пяти самых молодых пользователей."
      ],
      "metadata": {
        "id": "P1CLQW2h5vRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"UserAge\")\n",
        "user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
        "user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
        "for user_id, avg_age in youngest_users:\n",
        "\tprint(user_id, avg_age)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJH38Cd0h4A",
        "outputId": "79267c8f-870f-4df6-9192-8460e5a6bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 20.0\n",
            "6 22.0\n",
            "1 25.0\n",
            "5 28.0\n",
            "2 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "9dboHrId1aUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Найти среднюю цену товара в каждой категории и вывести результат в формате \"Категория: Средняя цена\"."
      ],
      "metadata": {
        "id": "EhM3A6aJ0nkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Создает экземпляр `SparkContext` с именем \"ProductPrice\", который используется для взаимодействия с кластером Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, а не на кластере.\n",
        "\n",
        "2. Создает RDD (Resilient Distributed Dataset) с именем `product_rdd`, используя метод `parallelize`. Этот RDD содержит кортежи, где каждый кортеж представляет собой тройку: идентификатор продукта, категорию продукта и цену продукта.\n",
        "\n",
        "3. Применяет функцию `map` к `product_rdd`, чтобы преобразовать каждый кортеж в пару ключ-значение, где ключом является категория продукта, а значением — кортеж, содержащий цену и количество единиц продукта (в данном случае всегда 1).\n",
        "\n",
        "4. Использует `reduceByKey` для агрегации значений по ключу (категории продукта). Функция `reduceByKey` принимает функцию, которая определяет, как агрегировать значения. В данном случае, функция суммирует цены и количество единиц продукта для каждой категории.\n",
        "\n",
        "5. После агрегации, `mapValues` преобразует значения в RDD, вычисляя среднюю цену для каждой категории, разделив общую сумму цен на количество единиц продукта.\n",
        "\n",
        "6. Наконец, `collect` собирает результаты в драйвер программы, и цикл `for` выводит каждую категорию и ее среднюю цену.\n",
        "\n",
        "В итоге, код вычисляет среднюю цену продуктов по категориям, используя функции `map`, `reduceByKey`, и `mapValues` для обработки и агрегации данных в RDD."
      ],
      "metadata": {
        "id": "4JSFZ_cz53wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"ProductPrice\")\n",
        "product_rdd = sc.parallelize([(1, \"A\", 100), (2, \"B\", 150), (3, \"A\", 120), (4, \"C\", 200), (5, \"B\", 130), (6, \"C\", 140)])\n",
        "category_total_price = product_rdd.map(lambda x: (x[1], (x[2], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "for category, avg_price in category_total_price.collect():\n",
        "\t  print(category, avg_price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqCBO7Dm0obi",
        "outputId": "f42b3864-f422-4709-a73a-2a4442dbb6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A 110.0\n",
            "B 140.0\n",
            "C 170.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "OIRMo2rI1azG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Найти все пары чисел из RDD, сумма которых превышает 100, и вывести их."
      ],
      "metadata": {
        "id": "AxxMQ7BI0wxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark:\n",
        "\n",
        "1. **Инициализация SparkContext**: Сначала создается экземпляр `SparkContext` с именем \"NumberPairs\", который будет использоваться для выполнения операций в Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, а не на кластере [4].\n",
        "\n",
        "2. **Создание RDD**: Затем создается RDD (Resilient Distributed Dataset) с именем `number_rdd`, используя метод `parallelize` на `SparkContext`. Этот метод преобразует обычный список чисел в распределенный набор данных, который может быть обработан параллельно. В данном случае, список содержит числа от 30 до 140 [0].\n",
        "\n",
        "3. **Операция Cartesian Product**: Далее, на `number_rdd` применяется операция `cartesian`, которая создает декартово произведение RDD с самим собой. Это означает, что для каждого элемента в `number_rdd` будет создана пара, где первый элемент пары - это исходный элемент, а второй - любой другой элемент из `number_rdd`. В результате получается RDD, содержащий все возможные пары чисел из исходного списка [2].\n",
        "\n",
        "4. **Фильтрация пар**: После создания декартового произведения, к RDD применяется фильтрация с помощью метода `filter`. Фильтр сохраняет только те пары чисел, сумма которых больше 100. Это достигается с помощью лямбда-функции, которая принимает пару чисел и возвращает `True`, если сумма чисел в паре больше 100, и `False` в противном случае [2].\n",
        "\n",
        "5. **Сборка результатов**: Наконец, метод `collect` используется для сбора всех оставшихся после фильтрации пар чисел в массив на драйвере. Этот массив затем перебирается в цикле `for`, и каждая пара чисел выводится на экран [0].\n",
        "\n",
        "В итоге, код генерирует и выводит все пары чисел из исходного списка, сумма которых больше 100, используя возможности распределенных вычислений Apache Spark."
      ],
      "metadata": {
        "id": "Y0EDTNVs6Cx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"NumberPairs\")\n",
        "number_rdd = sc.parallelize([30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])\n",
        "number_pairs = number_rdd.cartesian(number_rdd).filter(lambda x: x[0] + x[1] > 100).collect()\n",
        "for pair in number_pairs:\n",
        "    print(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGYAuo7M0x3s",
        "outputId": "4ecaaf36-6c39-4d82-af52-a1ce02e551d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 80)\n",
            "(30, 90)\n",
            "(30, 100)\n",
            "(30, 110)\n",
            "(30, 120)\n",
            "(30, 130)\n",
            "(30, 140)\n",
            "(40, 70)\n",
            "(40, 80)\n",
            "(40, 90)\n",
            "(40, 100)\n",
            "(40, 110)\n",
            "(40, 120)\n",
            "(40, 130)\n",
            "(40, 140)\n",
            "(50, 60)\n",
            "(50, 70)\n",
            "(50, 80)\n",
            "(50, 90)\n",
            "(50, 100)\n",
            "(50, 110)\n",
            "(50, 120)\n",
            "(50, 130)\n",
            "(50, 140)\n",
            "(60, 50)\n",
            "(60, 60)\n",
            "(60, 70)\n",
            "(60, 80)\n",
            "(60, 90)\n",
            "(60, 100)\n",
            "(60, 110)\n",
            "(60, 120)\n",
            "(60, 130)\n",
            "(60, 140)\n",
            "(70, 40)\n",
            "(70, 50)\n",
            "(70, 60)\n",
            "(70, 70)\n",
            "(70, 80)\n",
            "(70, 90)\n",
            "(70, 100)\n",
            "(70, 110)\n",
            "(70, 120)\n",
            "(70, 130)\n",
            "(70, 140)\n",
            "(80, 30)\n",
            "(80, 40)\n",
            "(80, 50)\n",
            "(80, 60)\n",
            "(80, 70)\n",
            "(80, 80)\n",
            "(80, 90)\n",
            "(80, 100)\n",
            "(80, 110)\n",
            "(80, 120)\n",
            "(80, 130)\n",
            "(80, 140)\n",
            "(90, 30)\n",
            "(90, 40)\n",
            "(90, 50)\n",
            "(90, 60)\n",
            "(90, 70)\n",
            "(90, 80)\n",
            "(90, 90)\n",
            "(90, 100)\n",
            "(90, 110)\n",
            "(90, 120)\n",
            "(90, 130)\n",
            "(90, 140)\n",
            "(100, 30)\n",
            "(100, 40)\n",
            "(100, 50)\n",
            "(100, 60)\n",
            "(100, 70)\n",
            "(100, 80)\n",
            "(100, 90)\n",
            "(100, 100)\n",
            "(100, 110)\n",
            "(100, 120)\n",
            "(100, 130)\n",
            "(100, 140)\n",
            "(110, 30)\n",
            "(110, 40)\n",
            "(110, 50)\n",
            "(110, 60)\n",
            "(110, 70)\n",
            "(110, 80)\n",
            "(110, 90)\n",
            "(110, 100)\n",
            "(110, 110)\n",
            "(110, 120)\n",
            "(110, 130)\n",
            "(110, 140)\n",
            "(120, 30)\n",
            "(120, 40)\n",
            "(120, 50)\n",
            "(120, 60)\n",
            "(120, 70)\n",
            "(120, 80)\n",
            "(120, 90)\n",
            "(120, 100)\n",
            "(120, 110)\n",
            "(120, 120)\n",
            "(120, 130)\n",
            "(120, 140)\n",
            "(130, 30)\n",
            "(130, 40)\n",
            "(130, 50)\n",
            "(130, 60)\n",
            "(130, 70)\n",
            "(130, 80)\n",
            "(130, 90)\n",
            "(130, 100)\n",
            "(130, 110)\n",
            "(130, 120)\n",
            "(130, 130)\n",
            "(130, 140)\n",
            "(140, 30)\n",
            "(140, 40)\n",
            "(140, 50)\n",
            "(140, 60)\n",
            "(140, 70)\n",
            "(140, 80)\n",
            "(140, 90)\n",
            "(140, 100)\n",
            "(140, 110)\n",
            "(140, 120)\n",
            "(140, 130)\n",
            "(140, 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "lXAOBptO1bZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Домашнее задание:\n",
        "Условие: Найти самую длинную последовательность упорядоченных чисел в RDD и вывести её"
      ],
      "metadata": {
        "id": "r7_jUYja01Zv"
      }
    }
  ]
}